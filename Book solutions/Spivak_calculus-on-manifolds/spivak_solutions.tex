% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\pagestyle{fancy}

 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\qed}{\hfill$\blacksquare$}
\let\newproof\proof
\renewenvironment{proof}{\begin{addmargin}[1em]{0em}\begin{newproof}}{\end{newproof}\end{addmargin}\qed}
% \newcommand{\expl}[1]{\text{\hfill[#1]}$}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------

\lhead{Spivak ``Calculus on Manifolds'' Solutions}
\chead{Stanley Tuznik}
\rhead{\today}
 
% \maketitle


\section*{Chapter 1}


\begin{problem}{1-1} Prove that $\left|x\right| \leq \sum_{i=1}^n \left| x^i\right|$.
\end{problem}

\begin{proof}
On the face of it, this seems to resemble the triangle inequality. However, notice that the $\left| \cdot \right|$ is a vector magnitude and the same symbol on the right represents an absolute value of a real number. However, we can use the triangle inequality here:
\begin{equation*}
\left| x \right|^2  = \left| x^1 e_1 + x^2 e_2 + \cdots + x^n e_n \right|^2
\end{equation*}
Where we have expanded $x$ in the usual basis of $\mathbb{R}^n$. From the triangle inequality applied to this vector sum, we have
\begin{equation*}
\left|x\right|^2 \leq \left(\sum_{i=1}^n \left|x^i\right| \left| e_i \right| \right)^2 = \left(\sum_{i=1}^n \left|x^i \right|\right)^2
\end{equation*}
Taking the square root, the statement is obtained.
\end{proof}



\begin{problem}{1-2}
When does equality hold in Theorem 1-1(3)? \textit{Hint}: Re-examine the proof; the answer is not ``when $x$ and $y$ are linearly dependent.''
\end{problem}
\begin{proof}
In the proof, the key inquality assumption we make is that $$ \sum_{i=1}^n x^i y^i \leq \left|x\right|\left|y\right| $$ which follows from the Cauchy-Schwarz inequality of 1-1(2). Equality will hold for the triangle inequality if and only if equality holds for this inequality. 

If equality holds, then we have $ \sum_{i=1} x^i y^i = \left|x\right|\left|y\right|$, and so $\left|\sum_{i=1} x^i y^i \right| = \left|x\right|\left|y\right|$, and by 1-1(2) we have that $x$ and $y$ are linearly dependent. 

In the other direction, assume that $x$ and $y$ are linearly dependent, i.e., there is some $\lambda \in \mathbb{R}\setminus \left\{0\right\}$ with $y = \lambda x$. Then the key inequality looks like $$ \lambda \left|x\right|^2   = \sum_{i=1}^n x^i y^i \leq \left|x\right|\left|y\right| = \left|\lambda\right| \left|x\right|^2 $$ so that equality holds iff $ \lambda = \left|\lambda\right|$, i.e., $\lambda >0$. This means that $x$ and $y$ ``point in the same direction.''
\end{proof}





\begin{problem}{1-3}
Prove that $\left|x-y\right| \leq \left|x\right| + \left|y\right|$. When does equality hold?
\end{problem}
\begin{proof}
Let $x,y \in \mathbb{R}^n$. Then we have
$$ \left|x-y\right| = \left|x + \left(-y\right)\right| \leq \left|x\right| + \left| -y\right| = \left|x\right| + \left| y\right| $$ This is an easy application of the triangle inequality.
\end{proof}



\begin{problem}{1-4}
Prove that $\left| \left|x\right|- \left|y\right| \right| \leq \left|x-y\right|$. 
\end{problem}
\begin{proof}
This is commonly referred to as the reverse triangle inequality. It is rather easily seen as a corollary to the standard triangle inequality. Let $x,y \in \mathbb{R}^n$. Then note that
$$ \left| x\right| = \left| \left(x-y\right) + y \right| \leq \left|x-y\right| + \left|y\right| $$ Upon subtraction, we obtain
$$ \left|x\right| - \left|y\right| \leq \left|x-y\right| $$
Repeating this but starting with $y$, we obtain the similar
$$ - \left( \left|x\right| - \left|y\right| \right) \leq \left|x-y\right| $$
Taken together, these last two results give
$$ \left| \left|x\right| - \left|y\right| \right| \leq \left|x-y\right| $$
\end{proof}






\begin{problem}{1-5}
The quantity $\left|y-x\right|$ is called the distance between $x$ and $y$. Prove and interpret geometrically the ``triangle inequality'' $\left|z-x\right| \leq \left|z-y\right| + \left|y-x\right|$.
\end{problem}
\begin{proof}
Let $x,y,z \in \mathbb{R}^n$. Then
\begin{equation*}
\begin{split}
\left|z-x\right| & = \left| \left(z-y\right) + \left(y-x\right) \right| \\
& \leq \left|z-y\right| + \left|y-x\right| \\
\end{split}
\end{equation*}
Note that the trick of simultaneously adding and subtracting a quantity --- a net addition of 0 --- is a standard trick that occurs quite often in mathematics.

The moniker ``triangle inequality'' is obvious if we interpret $x$, $y$, and $z$ as vertices of a triangle. Then $\left|z-x\right|$, $\left|z-y\right|$, and $\left|y-x\right|$ are the three pairwise distances between these points, i.e., the lengths of the sides of the triangle. This statement says that the length of any side of a triangle is \textit{at most} the sum of the other two sides. This makes sense if we just think about constructing triangles; it seems quite impossible to create a triangle with one side longer than both other sides combined. 
\end{proof}



\begin{problem}{1-6}
Let $f$ and $g$ be integrable on $\left[a,b\right]$.
\begin{itemize}
	\item Prove that $\left|\int_a^b f\cdot g \right| \leq \left(\int_a^b f^2\right)^{1/2}\cdot \left(\int_a^b g^2\right)^{1/2}$. \textit{Hint}: Consider separately the cases $0 = \int_a^b \left(f-\lambda g\right)^2$ for some $\lambda \in \mathbb{R}$ and $0 < \int_a^b \left(f-\lambda g\right)^2$ for all $\lambda \in \mathbb{R}$.
\end{itemize}
\end{problem}
\begin{proof}
\begin{itemize}
	\item As the hint suggests, we first consider the case $0 = \int_a^b \left(f-\lambda g\right)^2$ for some $\lambda$. Multiplying out the integrand, we have $$ 0 = \int_a^b \left(f^2 - 2\lambda f\cdot g + \lambda^2 g^2 \right) =  \left( \int_a^b g^2\right) \lambda^2 - 2\left(\int_a^b f\cdot g\right) \lambda + \left(\int_a^b f^2\right) $$ This is, again, a quadratic equation in $\lambda$, and the equality states that this quadratic function has a root at this particular $\lambda$. That is, assume $\int_a^b g^2 \neq 0$, we have $$ \lambda = \frac{\left(\int_a^b f\cdot g\right) \pm \sqrt{\left(\int_a^b f\cdot g\right)^2 - \left(\int_a^b f^2\right)\left(\int_a^b g^2\right) }}{\left(\int_a^b g^2 \right)} $$ In order for this real root $\lambda$ to exist, we must have the discriminant non-negative, i.e., $$\left(\int_a^b f\cdot g\right)^2 - \left(\int_a^b f^2\right)\left(\int_a^b g^2\right) \geq 0 $$ which, upon taking the square root, is what we sought to prove.
\end{itemize}
\end{proof}





\begin{problem}{1-7}
A linear transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is norm preserving if $\left| T\left(x\right) \right| = \left| x \right|$, and inner product preserving if $\left\langle T\left(x\right), T\left(y\right) \right\rangle = \left\langle x,y\right\rangle$.
\begin{itemize}
	\item Prove that $T$ is norm preserving if and only if $T$ is inner product preserving. \\
	\item Prove that such a linear transformation $T$ is 1-1 and $T^{-1}$ is of the same sort.\\
\end{itemize}
\end{problem}

\begin{proof}
\begin{itemize}
	\item Let $T$ be norm preserving. Recall that the polarization identity relates the inner product with the norm. Thus, by the polarization identity,
	\begin{equation*}
	\begin{split}
	\left\langle T\left(x\right), T\left(y\right) \right\rangle & = \frac{\left|T\left(x\right) + T\left(y\right) \right|^2 - \left|T\left(x\right)-T\left(y\right)\right|^2 }{4} \\
	& = \frac{\left| T\left(x+y\right)\right|^2 - \left| T\left(x-y\right)\right|^2}{4} \\
	& = \frac{\left| x+y\right|^2 - \left|x-y\right|^2}{4}  \\
	& = \left\langle x,y\right\rangle 
	\end{split} 
	\end{equation*}
	hence it is inner product preserving. \\
	Conversely, let $T$ be inner product preserving. Then
	$$ \left|T\left(x\right)\right|^2 = \left\langle T\left(x\right), T\left(x\right) \right\rangle = \left\langle x, y\right\rangle = \left|x\right|^2 $$ and so it is norm preserving. \\
	\item Let $T$ be such a linear transformation. To show that it is injective, let $x,y\in \mathbb{R}^n$ such that $T\left(x\right) = T\left(y\right)$. Then we want to show that $x$ is equal to $y$. Thus, consider the distance between these points:
	$$ \left|x-y\right| = \left| T\left(x-y\right) \right| = \left|T\left(x\right) - T\left(y\right) \right| = 0$$
	by the norm preserving property and linearity of $T$. Hence, $x=y$ and $T$ is injective. Let $u\in \mathbb{R}^n$ be some element in the range of $T$. Then since $T$ is injective, we can find some $x\in\mathbb{R}^n$ such that $T\left(x\right) = u$, i.e., $x = T^{-1}\left(u\right)$ (basically, $T$ is surjective on its range). We want to consider the properties of $T^{-1}$ (we are looking at a specific point where we are saying it exists --- is there some general theorem about linear injective maps which guarantees its existence?). Well,
	$$ \left| u \right| = \left| T \circ T^{-1} \left(u\right) \right| = \left| T \left(T^{-1}\left(u\right)\right) \right| = \left| T^{-1}\left(u\right) \right|, $$ i.e., $T^{-1}$ is norm-preserving, and we are done.
\end{itemize}
\end{proof}



\begin{problem}{1-8}
If $x,y \in \mathbb{R}^n$ are non-zero, the angle between $x$ and $y$, denoted $\angle\left(x,y\right)$, is defined as $\arccos \left( \left\langle x,y\right\rangle / \left|x\right| \cdot \left|y\right| \right)$, which makes sense by Theorem 1-1 (2). The linear transformation $T$ is angle preserving if $T$ is 1-1, and for $x,y \neq 0$ we have $\angle \left(Tx,Ty\right) = \angle\left(x,y\right)$.
\begin{itemize}
	\item Prove that if $T$ is norm preserving, then $T$ is angle preserving. \\
	\item If there is a basis $x_1,\ldots,x_n$ of $\mathbb{R}^n$ and numbers $\lambda_1,\ldots, \lambda_n$ such that $Tx_i = \lambda_ix_i$, prove that $T$ is angle preserving if and only if all $\left| \lambda_i \right|$ are equal. \\
	\item What are all angle preserving $T:\mathbb{R}^n \rightarrow \mathbb{R}^n$?
\end{itemize}
\end{problem}

\begin{proof}

\begin{itemize}
	\item Let $T$ be norm preserving. Then we know from the previous problem that it is also inner product preserving. Hence,
	\begin{equation*}
	\begin{split}
	\angle \left( Tx,Ty\right) & = \arccos \left( \left\langle Tx,Ty \right\rangle / \left|Tx\right| \cdot \left| Ty \right| \right) \\ 
	& = \arccos \left( \left\langle x, y\right\rangle / \left|x\right| \cdot \left| y\right| \right) \\
	& = \angle \left(x,y\right) \\
	\end{split}
	\end{equation*}
Hence $T$ is angle preserving. \\
	\item Let $\left\{ x_i \right\}_{i=1}^n$ be such a basis and $\left\{ \lambda_i \right\}_{i=1}^n$ be the numbers in the hypothesis. For one direction, assume that $\left|\lambda_i\right| = \alpha > 0$ for all $i=1,2,\ldots,n$ (if $\lambda = 0$, then $Tx = 0$ for all $x\in\mathbb{R}^n$). Then since $\left\{x_i\right\}_{i=1}^n$ is a basis, we can write an arbitrary vector $v\in \mathbb{R}^n$ as $$ v = v^1 x_1 + \ldots + v^n x_n $$ so that $$ Tv = v^1 Tx_1 + \ldots + v^n Tx_n = \alpha \left(v^1 x_1 + \ldots + v^n x_n\right) = \alpha v. $$ That is, this $T$ merely scales vectors uniformly in all directions. Then we have $$ \frac{\left\langle Tx, Ty \right\rangle }{\left|Tx\right| \left|Ty\right|} = \frac{\alpha^2 \left\langle x, y\right\rangle}{\alpha^2 \left|x\right|\left|y\right|} = \frac{\left\langle x, y\right\rangle}{\left|x\right|\left|y\right|}  $$ so that $T$ is clearly angle-preserving.
	
	Conversely, assume $T$ is angle-preserving. Then we know that it is also norm-preserving. Thus, consider applying $T$ to the basis vectors $x_i$; we have $\left|Tx_i\right| = \left|x_i\right|$, and by the hypothesis, this means that $$\left|x_i\right| = \left|Tx_i\right| = \left|\lambda_i x_i\right| = \left|\lambda_i\right| \left|x_i\right| $$ so that if $\left|x_i\right| \neq 0$, we have $\lambda_i = 1$ for every $i$. \\
	\item Notice that we have considered angle preserving transformations on $\mathbb{R}^N$ which scale each basis vector uniformly. This makes senses, since these scalings would not affect angles. On the other hand, angle-preserving transformations include rotations as well as scaling (no translations!).
\end{itemize}

\end{proof}






\begin{problem}{1-9}
If $0 \leq \theta < \pi$, let $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ have the matrix $\left( \begin{array}{rr} \cos \theta & \sin \theta \\ -\sin \theta & \cos \theta \end{array} \right)$. Show that $T$ is angle preserving and if $x\neq 0$, then $\angle \left(x, Tx\right) = \theta $.
\end{problem}

\begin{proof}
We could show that this matrix is angle preserving through brute force application of the definition: let $x,y \in \mathbb{R}^2$ arbitrary and show that $\left\langle Tx,Ty\right\rangle$ and $\left\langle x,y \right\rangle$ return the same value. Alternately, we can consider just a single vector $x\in \mathbb{R}^n$ and show that $T$ is norm preserving, since we know from the problem 1-7 that these are equivalent.

Let $x = \left(x_1,x_2\right)^T \in \mathbb{R}^2$. Then 
\begin{equation*}
\begin{split}
\left| T\left(x\right)\right|^2 & = \left(x_1 \cos \theta + x_2 \sin \theta \right)^2 + \left(-x_1 \sin \theta + x_2 \cos \theta \right)^2  \\ 
& = \left(\cos^2 \theta + \sin^2 \theta \right) x_1^2 + \left(cos^2 \theta + \sin^2 \theta \right) x_2^2 + 2\left(\cos \theta \sin \theta - \cos \theta \sin \theta \right) x_1 x_2 \\
& = x_1^2 + x_2^2 \\
& = \left|x\right|^2
\end{split}
\end{equation*}
Thus, $T$ is norm preserving and so also angle preserving. 

To look at the angle between $x$ and $T\left(x\right)$, we must look at the arc cosine of the following quantity:
\begin{equation*}
\begin{split}
\frac{\left\langle x,Tx\right\rangle}{\left|x\right|\left|Tx\right|} & = \frac{x_1^2 \cos \theta +\left(\sin \theta - \sin \theta\right)x_1x_2 + x_2^2 \cos \theta}{\left|x\right|^2}\\
& = \frac{\left(x_1^2 + x_2^2\right) \cos \theta}{\left|x\right|^2} \\
& = \cos \theta \\
\end{split}
\end{equation*}
Hence, the angle between $x$ and $Tx$ is
\begin{equation*}
\angle \left(x,Tx\right) = \arccos \left( \frac{\left\langle x, Tx\right\rangle }{\left|x\right|\left|Tx\right|} \right) = \arccos \left(\cos \theta \right) = \theta 
\end{equation*}
\end{proof}



 
\begin{problem}{1-10} 
If $T:\mathbb{R}^m \rightarrow \mathbb{R}^n$ is a linear transformation, show that there is a number $M$ such that $\left| T\left(h\right)\right| \leq M \left|h\right|$ for $h \in \mathbb{R}^m$. \textit{Hint}: Estimate $\left|T\left(h\right)\right|$ in terms of $\left|h\right|$ and the entries in the matrix of $T$.
\end{problem}
 
\begin{proof}
The statement is intuitively obvious: it makes sense that a linear map between two finite-dimensional vector spaces cannot just take a vector anywhere, but instead is bounded above by some scalar multiple of (the magnitude of) that vector. Keep in mind that we can write each component of the matrix-vector product $T\left(h\right) = T h$ as a sum (inner product) of a particular row of $T$ with $h$:
$$\left[ Th\right]_j = T_{j*} h = T_{*j}^Th = \left\langle T_{*j}, h\right\rangle $$ With this in hand, the squraed Euclidean norm of the product $Th$ is a sum of squares of these inner products: $$ \left| Th\right|^2 = \sum_{i=1}^n  \left\langle T_{*j}, h\right\rangle^2 $$ We can use the Cauchy-Schwarz inequality to bound the inner product on the right:
\begin{equation*}
\begin{split}
\left| Th\right|^2 & = \sum_{i=1}^n  \left\langle T_{*j}, h\right\rangle^2  \\
& \leq \sum_{i=1}^n \left( \left|T_{*j}\right|\left|h\right|\right)^2 \\
& = \sum_{i=1}^n \left|T_{*j}\right|^2 \left|h\right|^2 \\
& = \left|h\right|^2 \left(\sum_{i=1}^n \left|T_{*j}\right|^2\right) \\
& = M^2 \left|h\right|^2
\end{split}
\end{equation*}
where we have defined $M^2$ as the quantity in parentheses. Then, taking the square root, we have $$ \left|Th\right| \leq M\left|h\right|. $$
\end{proof}
 
 
 
 
\begin{problem}{1-11}
If $x,y\in \mathbb{R}^n$ and $z,w\in \mathbb{R}^m$, show that $\left\langle \left(x,z\right),\left(y,w\right) \right\rangle = \left\langle x,y \right\rangle + \left\langle z,w\right\rangle$ and $\left|\left(x,z\right)\right| = \sqrt{\left|x\right|^2+\left|z\right|^2}$. Note that $\left(x,z\right)$ and $\left(y,w\right)$ denote points in $\mathbb{R}^{n+m}$.
\end{problem}
 
\begin{proof}
This problem shows that the inner product and the norm behave together well (note that the Euclidean 2-norm is actually induced by the standard Euclidean inner product; this is converse to the exposition provided in this book). First, note that the $i$th component of $\left(x,z\right)$ is
\begin{equation*}
\left(x,z\right)^i = \left\{ \begin{array}{lr}
x^i, & i=1,2,\ldots,n \\
z^{i-n}, & i = n+1,n+2,\ldots,n+m
\end{array} \right.
\end{equation*}
and similarly for $\left(y,w\right)$.

\begin{equation*}
\begin{split}
\left\langle \left(x,z\right),\left(y,w\right) \right\rangle & = \sum_{i=1}^{n+m} \left(x,z\right)^i \left(y,w\right)^i \\
& = \sum_{i=1}^n x^i y^i + \sum_{i=n+1}^{n+m} z^{i-n} w^{i-n} \\
& = \sum_{i=1}^n x^i y^i + \sum_{j=1}^m z^j w^j \\
& = \left\langle x, y\right\rangle + \left\langle z,w\right\rangle \\
\end{split}
\end{equation*}
where a change of summation index was used to simplify the second summation. Hence the first statement holds.

Next, we can use the same trick:
\begin{equation*}
\begin{split}
\left| \left(x,z\right)\right|^2 & = \sum_{i=1}^{n+m} \left(\left(x,z\right)^i\right)^2 \\
& = \sum_{i=1}^n \left(x^i\right)^2 + \sum_{i=n+1}^{n+m} \left(z^{i-n}\right)^2 \\
& = \sum_{i=1}^n \left(x^i\right)^2 + \sum_{j=1}^m \left(z^j\right)^2 \\
& = \left|x\right|^2 + \left|z\right|^2 \\
\end{split}
\end{equation*}
and, taking the square root of both sides, the statement holds.
\end{proof}
 
 
 
 \begin{problem}{1-12} Let $\left(\mathbb{R}^n\right)^*$ denote the dual space of the vector space $\mathbb{R}^n$. If $x\in\mathbb{R}^n$, define $\phi_x \in \left(\mathbb{R}^n\right)^*$ by $\phi_x \left(y\right) = \left\langle x, y\right\rangle$. Define $T:\mathbb{R}^n \rightarrow \left(\mathbb{R}^n\right)^*$ by $T\left(x\right) = \phi_x$. Show that $T$ is a 1-1 linear transformation and conclude that every $\phi \in \left(\mathbb{R}^n\right)^*$ is $\phi_x$ for a unique $x\in \mathbb{R}^n$.
\end{problem}
 
 \begin{proof}
 Given a point $x\in \mathbb{R}^n$, we can define a dual vector space element $\phi_x$ which is (by definition of the dual space) a linear function from $\mathbb{R}^n$ to $\mathbb{R}$. The problem statement defines a mapping $T$ which actually performs this assignment, i.e., it takes a point in $x \in \mathbb{R}^n$ and returns the dual vector $T\left(x\right) = \phi_x$. We want to show that this mapping $T$ assigning dual vectors to vectors in the obvious way, through the inner product, is linear and injective. The last statement of the proof is that of surjectivity, and taken together we are showing that $\mathbb{R}^n$ and $\left(\mathbb{R}^n\right)^*$ are isomorphic (AKA a linear bijection).
 
 First, let $x,y \in \mathbb{R}^n$ and $\alpha, \beta \in \mathbb{R}$. Then $T$ gives
$$ T\left(\alpha x + \beta y \right) = \phi_{\alpha x + \beta y} $$
which is, itself, a dual vector, mapping $\mathbb{R}^n$ to $\mathbb{R}$ linearly, through the inner product construction. To further analyze it, let's see how it operates on an arbitrary element $z \in \mathbb{R}^n$: 
\begin{equation*}
\begin{split}
T\left(\alpha x + \beta y\right)\left(z\right) & = \phi_{\alpha x + \beta y} \left(z\right) \\
& = \left\langle \alpha x + \beta y, z \right\rangle \\
& = \alpha \left\langle x,z\right\rangle + \beta \left\langle y,z\right\rangle \\
& = \alpha \phi_x \left(z\right) + \beta \phi_y \left(z\right) \\
& = \alpha T\left(x\right) \left(z\right) + \beta T\left(y\right)\left(z\right) \\
& = \left[ \alpha T\left(x\right) + \beta T\left(y\right) \right] \left(z\right) \\
\end{split}
\end{equation*}
where we used the linearity of the inner product and the definitions of $\phi$ and $T$. Thus, since $z\in \mathbb{R}^n$ arbitrary, we conclude from this last that $T$ is linear. 

To prove injectivity, let $x,y\in \mathbb{R}^n$ be two elements in the domain of $T$ such that $T\left(x\right) = T\left(y\right)$. We must show that $x=y$. Note that $T\left(x\right) = \phi_x $ and $T\left(y\right) = \phi_y $, both elements of the dual vector space $\left(\mathbb{R}^n\right)^*$. Since these linear maps (dual vectors) are equal, they must agree on every vector in $\mathbb{R}^n$; that is,
$$ \phi_x \left(z\right) = \phi_y\left(z\right) \; \forall z\in \mathbb{R}^n $$
In particular, they must agree on the standard Euclidean basis. Let $e_i = \left(0,\ldots,1\ldots,0\right)^T$ be the $i$th basis vector, all zeroes except for the $i$th component being 1. Then
$$ \phi_x \left(e_i\right) = \left\langle x, e_i \right\rangle = x^i $$
and 
$$ \phi_y \left(e_i\right) = \left\langle y, e_i \right\rangle = y^i $$
Since these must be equal, we have $x^i = y^i$. Repeating this for every $i = 1,2,\ldots,n$, we see that $x=y$. Thus, $T$ is injective.

The remaining part of the problem is to show that this construction is more general, in the sense that any dual vector in $\left( \mathbb{R}^n\right)^*$ is one of these $\phi$ corresponding to some $x\in \mathbb{R}^n$.  That is, any dual vector is equivalent to an inner product with some element of $\mathbb{R}^n$, and that this particular element is unique.

Let $\phi \in \left(\mathbb{R}^n\right)^*$, and $z\in \mathbb{R}^n$. Recall that we know how a linear map acts on an arbitrary vector if we know how it acts on the basis vectors. This is due to linearity: writing $z = \sum_{i=1}^n z^i e_i $, we have
\begin{equation*}
\begin{split} 
\phi \left(z\right) & = \phi \left( \sum_{i=1}^n z^i e_i \right) \\
& = \sum_{i=1}^n z^i \phi \left(e_i\right) \\
\end{split}
\end{equation*}
Note that each of the values $\phi\left(e_i\right)$ are constant, independent of the $z \in \mathbb{R}^n$ we are operating on. This summation is of the form of a Euclidean inner product. Specifically,
\begin{equation*}
\phi\left(z\right) = \left\langle x, z \right\rangle = \phi_x \left(z\right)
\end{equation*}
where $x = \left( \phi\left(e_1\right), \phi\left(e_2\right), \ldots, \phi\left(e_n\right) \right)$. Thus we have identified an $x\in \mathbb{R}^n$ giving rise to our arbitrary dual vector $\phi$ through this inner product construction. Uniqueness of this $x$ follows as before, by operating this dual vector on the standard basis vectors. (NOTE: we can probably trivialize this part of the proof via some theorem on linear functions and injectivity implying surjectivity.)

We have shown that the function $ T:\mathbb{R}^n \rightarrow \left(\mathbb{R}^n\right)^*$ is a linear bijection, or isomorphism, between $n$-dimensional Euclidean space and its dual space.
\end{proof}
 
 
 
 
 
 
 \begin{problem}{1-13}
 If $x,y\in \mathbb{R}^n$, then $x$ and $y$ are called perpendicular (or orthogonal) if $\left\langle x, y\right\rangle = 0$. If $x$ and $y$ are perpendicular, prove that $\left|x+y\right|^2 = \left|x\right|^2 + \left|y\right|^2$.
\end{problem} 
\begin{proof}
Since the definition of orthogonality requires the inner product, and the target of our prove involves norms, we can easily prove this proposition by expressing our norm of interest in terms of inner products:
\begin{equation*}
\begin{split}
\left| x+y\right|^2 & = \left\langle x+y, x+y \right\rangle \\
& = \left\langle x \right\rangle + 2\left\langle x,y \right\rangle + \left\langle y ,y \right\rangle \\ 
& = \left|x\right|^2 + 2\cdot 0 + \left|y\right|^2 \\
& = \left|x\right|^2 + \left| y \right|^2\\
\end{split}
\end{equation*}
\end{proof}
 
 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}{1-14}
Prove that the union of any (even infinite) number of open sets is open. Prove that the intersection of two (and hence of finitely many) open sets is open. Give a counterexample for infinitely many open sets.
\end{problem}
\begin{proof} 
Note that in a general topological space, these properties are axioms of the topology; here, Spivak defines openness in terms of the basis of open rectangles, and leaves us to derive the properties of the topology. Let $U = \cup_{i=1}^{\infty} U_i$ be a union of countably many open sets. To show that it is open, let $x \in U$. Then $x \in U_i$ for all $i=1,2,\ldots,\infty$. By Spivak's definition, there is a rectangle $R_i$ so that $x \in R_i \subset U_i$ for each $i$. Then we have $\cup_{i=1}^{\infty} R_i \subset U$ and this union is certainly a rectangle (maybe an infinite one), so that $U$ is open.

Let $U$ and $V$ be open sets, and let $x \in U\cap V$. Then again by Spivak's definition, there are rectangles $R$ and $R'$ so that $x \in R \subset U$ and $x \in R' \subset V$. Certainly the intersection of two rectangles is a rectangle, and so $x \in R \cap R' \subset U\cap V$ so that the intersection of two open sets is open. By induction, it is easy to see that this works for the intersection of finitely many sets.

The issue with an infinite intersection is that things can get arbitrarily ``small.'' For example consider in $\mathbb{R}$ the sets $$ U= \left\{ \left(-\frac{1}{n}, \frac{1}{n}\right) \, : \, n\in\mathbb{N} \right\}. $$ Clearly each set is open as an open interval, and we have $$ \cap_{U_n \in U} U_n = \left\{ 0 \right\} $$ (this is easy to show). This singleton set is clearly not open, and so this is a counterexample.
\end{proof}

 
 
 
 \begin{problem}{1-15}
 Prove that $\left\{ x\in\mathbb{R}^n \, : \, \left|x-a\right| < r\right\}$ is open (see also Problem 1-27).
 \end{problem}
 \begin{proof}
 This set is the open ball of radius $r$ about $a$, often written as $B_r\left(a\right)$. To show openness, in Spivak's definition, we want to show that for any point $x\in B_r\left(a\right)$, we have some open rectangle $R$ with $x \in R \subset B_r\left(a\right)$.
 
 Let $x \in B_r\left(a\right)$. Note that $x$ and $a$ are both points in $\mathbb{R}^n$, and so have components: $x = \left(x_1,x_2,\ldots, x_n\right)$ and $a=\left(a_1,a_2,\ldots,a_n\right)$. It helps to draw the situation in 1D and 2D to get intuition. Let $$ \delta = r - \left|x-a\right|$$ so that $\delta$ is the radial distance from the point $x$ to the surface of the ball. We can imagine placing a corner of our rectangle halfway along this distance between the point $x$ and the surface of the ball, and this will define a rectangle contained entirely within the ball. To be precise, the body diagonal of the rectangle will be $$ r' = \left|x-a\right| + \frac{\delta}{2} = \frac{r + \left|x-a\right|}{2}$$ To obtain the lengths of the various sides of the rectangle, we need to project onto the various axes. This can be done by using similar triangles. Notice that with have $$ \sin \theta = \frac{x_i}{r} = \frac{x_i'}{r'} $$ and so the position of one coordinate of our rectangle is given by the scaling $x_i ' =\frac{r'}{r} x_i$. Hence, since $r' > r$, we have $x_i ' > x_i$, so that the point $x$ will be enclosed inside the rectangle $$ R = \left[-x_1', x_1'\right] \times\left[-x_2', x_2'\right]\times \cdots \times \left[-x_n', x_n'\right] $$
 \end{proof}
 
 
 
 
 
 
 
 
 
\begin{problem}{1-16}
Find the interior, exterior, and boundary of the sets 

	\begin{align*}
	& \left\{ x \in \mathbb{R}^n \, : \, \left|x\right| \leq 1 \right\} \\
	& \left\{ x \in \mathbb{R}^n \, : \, \left|x\right| = 1 \right\} \\
	& \left\{ x \in \mathbb{R}^n \, : \, \text{each } x^i \text{ is rational} \right\}. \\
	\end{align*}
\end{problem}
\begin{proof}
The first set is the closed unit ball in $\mathbb{R}^n$, $\overline{B_1\left(0\right)}$. The interior is the open unit ball, $B_1\left(0\right)$, and the exterior is $\mathbb{R}^n - \overline{B_1\left(0\right)}$, i.e., everything but this closed ball. The boundary is the surface of the ball, the set $S^{n-1} = \left\{ x \in \mathbb{R}^n \, : \, \left|x\right| = 1 \right\}$, since every open neighborhood of these points will contain points both inside the ball and points outside of the ball.

The second set is the surface of the ball, descriped in the previous example as $S^{n-1}$. This set has no interior (as a subset of $\mathbb{R}^n$), since every neighborhood of every point will necessarily intersect points outside of this surface. The exterior consists of both the interior of the unit ball as well as the exterior, that is, it is $ B_1\left(0\right) \cup \left(\mathbb{R}^n \setminus \overline{B_1\left(0\right)}\right)$. The surface is its own boundary, as already described.

Though this example is a bit more exotic, it is similar to the previous one. The set is the set of vectors in $\mathbb{R}^n$ with rational components, written here as $\mathbb{Q}^n$. Since the rationals are dense in the reals, any neighborhood about any $x \in \mathbb{Q}^n$ will contain points with real coordinates. Thus, $\mathbb{Q}^n$ is its own boundary, and no points are interior. Similarly, every neighborhood of a real number contains a rational number, so no point $y \notin \mathbb{Q}^n$ is contained in a neighborhood disjoint from $\mathbb{Q}^n$, hence the set has no exterior.
\end{proof} 
 
 
 
 
 
 
 
 
 
\begin{problem}{1-19}
If $A$ is a closed set that contains every rational number $r \in \left[0,1\right]$, show that $\left[0,1\right] \subset A$.
\end{problem}
\begin{proof}
Let $x \in \left[0,1\right]$ arbitrary. If $x$ rational, then $x\in A$. On the other hand, assume $x \notin \mathbb{Q}$. Assume by way of contradiction that $x \notin A$. Then since $A$ closed, there is some open neighborhood $U$ of $x$ which does not intersect $A$. Then $U$ being open means that there is some open interval $V$ with $x \in V \subset U$. Then since $x \in \left[0,1\right]$, the interval $V$ must intersect $\left[0,1\right]$. Since $V$ open, this intersection is also an interval, of the form $\left[0,b\right)$, $\left(b,1\right]$, or $\left(b,c\right)$, depending on if $x=0$, $x=1$, or $x\in \left(0,1\right)$, respectively. In any case, this new interval contains some rational number, since $\mathbb{Q}$ is dense in $\mathbb{R}$, and so we have the neighborhood $U$ of $x$ intersecting a point of $A$. However, this contradicts our assumption that $x$ was in the exterior of $A$. Thus, we must have $x \in A$, i.e., $\left[0,1\right] \subset A$.
\end{proof} 
 
 
 
 \begin{problem}{1-20}
 Prove the converse of Corollary 1-7: a compact subset of $\mathbb{R}^n$ is closed and bounded (see also Problem 1-28).
 \end{problem}
 \begin{proof}
 The proof will rely on the fact that $\mathbb{R}^n$ being a metric space gives us a collection of open sets to work with: the open balls; and the fact that compactness gives us a finite subcollection of open sets, given that we have a collection.
 
 Let $K\subset \mathbb{R}^n$ be compact. Fix $\epsilon>0$ and let $$\mathcal{O} = \left\{ B_{\epsilon}\left(x\right)\,: \, x\in K \right\}.$$ Then $\mathcal{O}$ is an open cover of $K$, and so there is a finite subcollection $$U = \left\{ B_{\epsilon}\left(x_{1}\right), B_{\epsilon}\left(x_{2}\right), \ldots, B_{\epsilon}\left(x_{k}\right) \right\} $$ which also covers $K$.
 
Then for any $x\in K$, we have $x \in B_{\epsilon}\left(x_n\right)$ for some $n \in \left\{1,2,\ldots,k\right\}$. Thus, $\left|x - x_n\right| \leq \epsilon$, and by the triangle inequality we have $\left|x\right| \leq \epsilon + \left|x_n\right|$. More generally, then, given $x \in K$ we have $$ \left|x\right| \leq \epsilon + \max \left\{ \left|x_1\right|,\left|x_2\right|, \ldots, \left|x_n\right| \right\} $$ so that $K$ is bounded.

This same trick gives us a separation from any point not in $K$. For instance, let $y \notin K$. Since $K$ is compact, define $r_x = \left|y-x\right|$ for every $x \in K$ and consider the open balls $B_{r_x/2}\left(x\right)$ which cover $K$. Note that $r_x > 0$ for every $x$, since $y \notin x$ and so the distance is non-zero, so this is well-defined. Then by compactness of $K$, there is a finite subcollection $B_{r_{x_k}/2}\left(x_k\right)$, $k=1,2,\ldots, n$ which covers $K$. Take $\delta = \frac{1}{2}\min_k \left\{ \left|y - {x_k}\right|\right\} $. Then we claim that $B_{\delta}\left(y\right)$ is an open ball which is disjoint entirely from the finite open cover of $K$, so it is also disjoint from $K$.

 \end{proof}
 
 
 
 
 
 
 
 
 
 
 
 
\begin{problem}{1-22}
If $U$ is open and $C\subset U$ is compact, show that there is a compact set $D$ such that $C \subset \text{interior} \, D$ and $D\subset U$.
\end{problem}
\begin{proof}

\end{proof}



 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
 
\begin{problem}{1.23}
If $f:A\rightarrow \mathbb{R}^m$ and $a\in A$, show that $\lim_{x\rightarrow a} f\left(x\right) = b$ if and only if $\lim_{x\rightarrow a} f^i\left(x\right) = b^i$ for $i=1,\ldots,m $.
\end{problem}
 
\begin{proof}
Assume first that $\lim_{x\rightarrow a} f\left(x\right) = b$.  Let $\epsilon > 0$. By definition of limit, there exists some $\delta' > 0$ such that $\left|x-a\right| < \delta '$ implies $\left|f\left(x\right) - b\right| < \epsilon $. Since 
$$ \left|f\left(x\right)-b\right|^2 = \sum_{i=1}^n \left| f^i\left(x\right) - b^i \right|^2 $$ is a sum of positive numbers, we have, 
$$ \left|f^i \left(x\right) -b^i \right| \leq \left|f\left(x\right)-b\right| $$ for every $i =1,2,\ldots,n$. Thus we have
$$ \left|x-a\right|< \delta ' \implies \left|f^i \left(x\right)-b^i\right| < \epsilon $$
That is, $\lim_{x\rightarrow a} f^i\left(x\right) = b^i $.

Next, assume $\lim_{x\rightarrow a} f^i\left(x\right)=b^i$ for all $i=1,2,\ldots,n$. Let $\epsilon >0$ be arbitrary. Then for any $i$, there exists some $\delta_i > 0$ such that $$ \left|x-a\right| < \delta_i \implies \left|f^i\left(x\right)-b^i\right| \leq \frac{\epsilon}{\sqrt{n}} $$ Let $\delta = \min\left\{\delta_1,\delta_2,\ldots,\delta_n\right\}$. Then we have, for $\left|x-a\right|<\delta$, 
$$ \left|f\left(x\right)-b\right|^2 = \sum_{i=1}^n \left|f^i\left(x\right)-b^i\right|^2 \leq \sum_{i=1}^n \left(\frac{\epsilon}{\sqrt{n}}\right)^2 = n \frac{\epsilon^2}{n} = \epsilon^2 $$ Taking the square root of both sides (both positive), we have shown the existence of a $\delta >0$ such that
$$ \left|x-a\right|< \delta \implies \left|f\left(x\right)-b\right| < \epsilon $$
That is, $\lim_{x\rightarrow a} f\left(x\right) = b$.

\end{proof}
 
 
 
\begin{problem}{1.24}
Prove that $f:A\rightarrow \mathbb{R}^m$ is continuous at $a$ if and only if each $f^i$ is.
\end{problem} 
 
 \begin{proof}
 This proposition readily holds from the previous problem, 1-23, if we replace $b$ with $f\left(a\right)$.
 \end{proof}

 
\begin{problem}{1.25}
 Prove that a linear transformation $T:\mathbb{R}^n\rightarrow \mathbb{R}^m$ is continuous. \textit{Hint}: Use Problem 1-10.
\end{problem}

 \begin{proof}
 Recall that Problem 1-10 allowed us to bound a linear transformation above in the sense that we can find a number $M$ such that 
 $$ \left|T\left(h\right)\right| \leq M \left| h\right| \; \text{for} \; h \in \mathbb{R}^n$$
 
Let $a\in \mathbb{R}^n$ be arbitrary. We want to show that $T$ is continuous as $a$, i.e., that $\lim_{x\rightarrow a} T\left(x\right) = T\left(a\right)$. Let $\epsilon > 0$, arbitrary, and let $M$ be the (positive) number guaranteed by Problem 1-10. Then notice that
$$ \left| T\left(x\right) - T\left(a\right) \right| = \left| T\left(x-a\right)\right| \leq M \left|x-a\right|.$$ With this in hand, let $\delta = \epsilon / M$. Then we have 
$$ \left|x-a\right| < \delta \implies \left|T\left(x\right) - T\left(a\right)\right| < M \frac{\epsilon}{M} = \epsilon $$
Thus we have shown that $\lim_{x\rightarrow a} T\left(x\right) = T\left(a\right)$, that is, $T$ is continuous at $a$. Since $a\in \mathbb{R}^n$ arbitrary, $T$ is continuous everywhere.
 \end{proof}
 
 
 
 \begin{problem}{1.26}
 Let $A = \left\{ \left(x,y\right) \in \mathbb{R}^2: \; x>0 \; \text{and} \; 0<y<x^2 \right\}$.
 \begin{itemize}
 	\item Show that every straight line through $\left(0,0\right)$ contains an interval around $\left(0,0\right)$ which is in $\mathbb{R}^2-A$. \\
 	\item Define $f: \mathbb{R}^2\rightarrow \mathbb{R}$ by $f\left(x\right)=0$ if $x \notin A$ and $f\left(x\right)=1$ if $x\in A$. For $h\in \mathbb{R}^2$ define $g_h: \mathbb{R}\rightarrow \mathbb{R}$ by $g_h\left(t\right) = f\left(th\right)$. Show that each $g_h$ is continuous at $0$, but $f$ is not continuous at $\left(0,0\right)$.
 \end{itemize}
 \end{problem}
 
\begin{proof}
 \begin{itemize}
 	\item Drawing this set $A$ is very informative for the problem. Straight lines through the origin are of the form $y=mx$. If $m <=0$, the line obviously does not intersect $A$, so we will consider $m > 0$. Then we want an interval in $\mathbb{R}$ about the origin so that so that $y\left(x\right) = mx$ on this interval does not intersect $A$. This problem is worded a little strangely. 
 	
 	Note that the line intersects $A$ when $ mx = x^2$. Since the origin $x=0$ is not included, we can divide by a factor of $x$ to obtain $m = x$. Thus, the straight line $y=mx$ hits the upper boundary of the set $A$ at $x=m$. We expect that $x \in \left(-m,m\right) \implies \left\{ mx\right\}\cap A = \varnothing$. To see this, consider $0 < x < m$ and we have
 	\begin{align*}
 		mx - x^2 & = x \left(m -x\right)  > 0 \\
 	\end{align*}
 	so that the curves do not intersect on $\left(0,m\right)$. The sets obviously do not intersect on $\left(-m,0\right]$ since $A$ is not even defined there.
 	
 	\item Note that $f$ is the indicator function of the set $A$, and that $th$ is the parametrization of the line through the origin and the point $h$. Thus, with knowledge of the previous part of the problem in hand, we know that if the line has non-positive slope, then it will not intersect $A$, and so $g_h\left(t\right) = 0$ if the slope of the line between $\left(0,0\right)$ and $h$ is negative. Hence, if $h$ is in the second or fourth quadrant, $g \equiv 0$ which is obviously continuous at $0$. 
 	
 	Next, assume $h$ is in the first or third quadrant, so that the line through $\left(0,0\right)$ and $h$ has positive slope, say $m$. Then from the first part we know that there is some interval about $0$ where the line does not intersect $A$, i.e., where $f=0$. Then we have $\lim_{t\rightarrow 0} g_h\left(t\right) = 0$ since is is identically zero on some small interval about the origin, hence $g_h$ is again continuous at zero in any case. 
 	
 	On the other hand, note that $f\left(0,0\right) = 0$ since $\left(0,0\right)\notin A$. However, every neighborhood of $\left(0,0\right)$ in $\mathbb{R}^2$ will necessarily include points of $A$, so we can construct a sequence which approaches $\left(0,0\right)$ on which $f$ is always $1$. In particular, consider $\lim_{n\rightarrow \infty} f\left( \frac{1}{n}, \frac{1}{2n^2} \right)$. This sequence $\left(\frac{1}{n}, \frac{1}{2n^2}\right) \subset A$ since $0 < \frac{1}{2n^2} < \frac{1}{n^2} $, and $\lim_{n\rightarrow \infty} \left(\frac{1}{n}, \frac{1}{2n^2}\right) \rightarrow \left(0,0\right)$. However, we have $f\left(\frac{1}{n}, \frac{1}{2n^2}\right) = 1$ for all $n$, so $\lim_{n\rightarrow \infty} f\left(\frac{1}{n}, \frac{1}{2n^2}\right) \neq f\left(0,0\right)$, and so the function $f$ is not continuous at $\left(0,0\right)$.
 
 \end{itemize}
\end{proof}
 
 
 
 
 
 
\begin{problem}{1.27}
Prove that $\left\{ x\in \mathbb{R}^n: \left|x-a\right| < r \right\}$ is open by considering the function $f: \mathbb{R}^n\rightarrow \mathbb{R}$ with $f\left(x\right) = \left|x-a\right|$.
\end{problem}
 
\begin{proof}
First, we want to show that the function $f$ is continuous. We know that the function is continuous (but not smooth) in $\mathbb{R}^1$. First, consider the simpler function $$ g\left(x\right) = \left|x\right|. $$ Then at an arbitrary point $y\in \mathbb{R}^n$, we have $$ \left|g\left(x\right)-g\left(y\right)\right| = \left| \left|x\right| - \left|y\right|\right| \leq \left| x - y \right| $$ by the reverse triangle inequality. Thus, given arbitrary $\epsilon > 0$, we can take $\left|x-y\right| < \epsilon$ to prove continuity. Thus, we have shown that $g\left(x\right) = \left|x\right|$ is continuous at all $x \in \mathbb{R}^n$. It follows that $f\left(x\right) = \left|x-a\right|$ is also continuous on $\mathbb{R}^n$. 

We know that by the topological definition of continuity, the preimage of an open set under a continuous function is an open set. Thus, if we can write the open ball $B_r\left(a\right)$ as the preimage of a suitable open set under $f$, then we are done. Let $$ F = \left\{ y \in \mathbb{R} \, \, : \,0 < y < r \right\}=\left(0,r\right)$$ which is certainly an open set. Then $$ f^{-1}\left(F\right) = \left\{ x \in  \mathbb{R}^n \, : \, f\left(x\right) < r \right\} = B_r\left(a\right), $$ so that the ball is open.
\end{proof}
 
 
 
\begin{problem}{1-28}
If $A\subset \mathbb{R}^n$ is not closed, show that there is a continuous function $f:A\rightarrow \mathbb{R}$ which is unbounded. \textit{Hint}: If $x\in \mathbb{R}^n \setminus A$ but $x\notin \text{interior}\, \left(\mathbb{R}^n \setminus A\right)$, let $f\left(y\right) = 1/\left|y-x\right|$.
\end{problem}
\begin{proof}
If $A\subset \mathbb{R}^n$ not closed, then there is some point $x \notin A$ and every neighborhood of $x$ intersects $A$. Since we are in a metric space, we can use the open balls as well as the metric to create a sequence which approaches $x$ but whose function values blow up. As the hint suggests, take for each $n \in \mathbb{N}$ the set $U_n = B_{1/n}\left(x\right)$ and an arbitrary point $y \in U_n$. Define $f\left(y\right) = \left|y-x\right|^{-1}$. Clearly, as $n\rightarrow \infty$ we have $y_n \rightarrow x$ and $f\left(y_n\right) \rightarrow \infty$. Since $f$ is defined on all $y \in A$ (since $x\notin A$), it is continuous as the multiplicative inverse of a non-zero, continuous function.

To be precise, note that since $y_n \in U_n$, we have $\left|x-y_n\right| < 1/n$, so that given $\epsilon > 0$, we can take $N$ so that $ n > N \implies \frac{1}{n} < \epsilon$ and see that $y_n \rightarrow x$. We can use this sequence to show that $f$ is unbounded. Let $M>0$ be arbitrary. Then by sequence convergence, there is some natural $N$ so that $ n>N \implies \left|x-y_n\right| < \frac{1}{M}$. That is, $$ \frac{1}{M} < \frac{1}{\left|x-y_n\right|} = f\left(y_n\right) $$ so that we have points in $A$ at which $f$ is larger than any arbitrary number.
\end{proof}
 
 
 
 
 
 
 

\begin{problem}{1.29}
If $A$ is compact, prove that every continuous function $f:A\rightarrow \mathbb{R}$ takes on a maximum and a minimum value.
\end{problem}

\begin{proof}
Let $A$ be compact and $f:A\rightarrow \mathbb{R}$ continuous. Then we know that the continuous image of a compact set is compact (Theorem 1-9 in the book), and so $f\left(A\right)$ is compact. We also know that, in $\mathbb{R}^n$, the compact sets are exactly the closed and bounded sets (Corollary 1-7 and Problem 1-20), and so $f\left(A\right)$ is closed and bounded.

Being closed and bounded, $f\left(A\right) \subset \mathbb{R}$ contains its maximum and its minimum values (for example, if it did not, then the maximum value would be a supremum outside the set, and the set would be open). Hence, the statement holds.
\end{proof}

\begin{proof}
Assume by way of contradiction that $f\left(A\right)$ does not contain a maximum or a minimum value. Then $f\left(A\right)$ is unbounded, but this is a contradiction since the continuous image of a compact set is compact, i.e., closed and bounded. Thus, $f\left(A\right)$ must contain its maximum or minimum value. We can do this independently for both the maximum and minimum, obtaining both.
\end{proof} 
 
 
 
 
\begin{problem}{1.30}
Let $f:\left[a,b\right]\rightarrow \mathbb{R}$ be an increasing function. If $x_1,\ldots,x_n \in \left[a,b\right]$ are distinct, show that $\sum_{i=1}^n o\left(f,x_i\right) < f\left(b\right)-f\left(a\right)$.
\end{problem}

\begin{proof}
This proof is mostly a manipulation of the finite sum, similar to that of a telescoping finite series. First, assume that the points $x_i$ are in increasing order. Since each oscillation involves the limit as $\delta \rightarrow 0$, we can assume from the start that the $\delta$-neighborhoods about each $x_i$ are so small as to not intersect. To be precise, we can take $\delta = \frac{1}{2}\min_{i=1,2,\ldots,n-1} \left\{ x_{i+1}-x_i \right\}$. Additionally, we assume that $x_1 \neq a$ and $x_n \neq b$.

Next, since the function $f$ is increasing on $\left[a,b\right]$, we have that $$ M\left(x_i, f, \delta\right) = f\left(x_i + \delta\right).$$ Note that this is the supremum because we assume that $f$ is continuous. Similarly, $$ m\left(x_i,f,\delta\right) = f\left(x_i - \delta\right).$$ Since we chose $\delta$ to make the intervals disjoint, we have $$ M\left(x_i, f,\delta\right) \leq m\left(x_{i+1},f,\delta\right)$$ for all $i=1,2,\ldots, n$. Then looking at the sum, we have 
\begin{align*}
\sum_{i=1}^n o\left(f,x_i\right) & = \sum_{i=1}^n \lim_{\delta\rightarrow 0} \left[ M\left(x_i,f,\delta\right) - m\left(x_i,f,\delta\right)\right] \\ 
& = \lim_{\delta\rightarrow 0} \sum_{i=1}^n \left[ f\left(x_i + \delta\right) - f\left(x_i - \delta\right)\right] \\
& = \lim_{\delta\rightarrow 0} \left[ f\left(x_n + \delta\right) - f\left(x_1 - \delta\right) \right] + \lim_{\delta\rightarrow 0} \sum_{i=1}^{n-1} \left[ -f\left(x_{i+1} - \delta\right) + f\left(x_i + \delta\right)\right] \\
\end{align*}
Notice that the terms being summed are each negative. We will replace this sum by the quantity $-B$, $B>0$, to indicate this. Since $f$ is not assumed to be continuous, we cannot simply take these limits. If it were possible, the entire quantity would be zero and the theorem would be trivially proven. In general, since we assumed that $x_n \neq b$ and $x_1 \neq a$, we have $$ \lim_{\delta\rightarrow 0} f\left(x_n+\delta\right) = \lim_{h\rightarrow x_n^+} f\left(h\right) \leq f\left(b\right) $$ 
$$ \lim_{\delta\rightarrow 0} f\left(x_1-\delta\right) = \lim_{h\rightarrow x_1^-} f\left(h\right) \geq f\left(a\right)  $$
Thus we have 
$$ \sum_{i=1}^n o\left(f,x_i\right) \leq f\left(b\right) - f\left(a\right) - \lim_{\delta \rightarrow 0} B \leq f\left(b\right) - f\left(a\right), $$ proving the theorem. 

Lastly, we consider the situation where, for instance, $x_1 = a$. In this case, by the definition of the quantities $M$ and $m$, and the monotonicity of $f$, we have $$ M\left(x_1, f,\delta\right) = f\left(a+\delta\right)$$ $$ m\left(x_1,f,\delta\right) = f\left(a\right) $$ and the rest of the proof proceeds similarly, but we do not even need to worry about the one-sided limits that we ended up taking (so this is a little easier).

\end{proof} 
 
 
 
 
 
 
 
 
 
 
 
 
 
 \newpage
\section*{Chapter 2}

\begin{problem}{2.1}
Prove that if $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable at $a\in \mathbb{R}^n$, then it is continuous at $a$. \textit{Hint}: Use Problem 1-10.
\end{problem}

\begin{proof}
Recall that Problem 1-10 allowed us to bound a linear map above by a function of its matrix entries. Specifically, for linear map $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$, we have a positive number $M$ such that
$$ \left|T\left(h\right)\right| \leq M \left| h\right| $$
for all $h \in \mathbb{R}^n$. This may come in handy when talking about derivatives, since a derivative (evaluated at a point) is a linear map.

Assume $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ is differentiable at $a\in \mathbb{R}^n$. That is, there exists linear map $Df\left(a\right)$ such that 
$$ \lim_{h\rightarrow 0} \frac{\left|f\left(h+a\right)-f\left(a\right) - Df\left(a\right) \left(h\right)\right|}{\left|h\right|}=0 $$ The definition of continuity at $a$ involves the limit of the quantity $ \left| f\left(h+a\right) - f\left(a\right)\right|$, and since we know $f$ is differentiable at $a$, we can likely use this definition of differentiability. 

Notice that
\begin{equation*}\begin{split}
\lim_{h\rightarrow 0} \frac{\left|f\left(h+a\right)- f\left(a\right)\right|}{\left|h\right|} & = \lim_{h\rightarrow 0} \frac{\left| f\left(h+a\right) - f\left(a\right) - Df\left(a\right)\left(h\right) + Df\left(a\right)\left(h\right) \right|}{\left|h\right|} \\
& \leq \lim_{h\rightarrow 0} \frac{\left| f\left(h+a\right) - f\left(a\right) - Df\left(a\right)\left(h\right)\right|}{\left|h\right|}  + \lim_{h\rightarrow 0} \frac{\left|Df\left(a\right)\left(h\right)\right|}{\left|h\right|} \\
\end{split}\end{equation*}
Notice that the first term on the right-hand side is the limit in the definition of the derivative of $f$ at $a$, and since the derivative exists, this limit is $0$. If we let $M$ be the number guaranteed to exist by Problem 1-10 for the linear map $Df\left(a\right)$, then we have
\begin{equation*}
\begin{split}
\lim_{h\rightarrow 0} \frac{\left|f\left(h+a\right)- f\left(a\right)\right|}{\left|h\right|} & \leq 0 + \lim_{h\rightarrow 0} \frac{M \left|h\right|}{\left|h\right|} = M \\
\end{split}
\end{equation*}
We are almost there. Looking at the limit of interest, we have
\begin{equation*}
\begin{split}
\lim_{h\rightarrow 0} \left| f\left(h+a\right)-f\left(a\right)\right| & = \lim_{h\rightarrow 0} \frac{\left|f\left(h+a\right)-f\left(a\right)\right|}{\left|h\right|} \left|h\right| \\
& = \left( \lim_{h\rightarrow 0} \frac{\left|f\left(h+a\right)-f\left(a\right)\right|}{\left|h\right|} \right) \left(\lim_{h\rightarrow 0} \left|h\right| \right) \\
& \leq M \cdot 0\\
& = 0
\end{split}
\end{equation*}
and thus $\lim_{h\rightarrow 0} \left|f\left(h+a\right)-f\left(a\right)\right| = 0$, and so $f$ is continuous at $a \in \mathbb{R}^n$ if it is differentiable there.
\end{proof}






\begin{problem}{2.2}
A function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ is independent of the second variable if for each $x\in \mathbb{R}$ we have $f\left(x,y_1\right) = f\left(x,y_2\right)$ for all $y_1,y_2 \in \mathbb{R}$. Show that $f$ is independent of the second variable if and only if there is a function $g:\mathbb{R}\rightarrow \mathbb{R}$ such that $f\left(x,y\right) = g\left(x\right)$. What is $f'\left(a,b\right)$ in terms of $g'$?
\end{problem}

\begin{proof}
Assume $f$ is independent of the second variable. Then we can define $g:\mathbb{R}\rightarrow \mathbb{R}$ as $g\left(x\right) = f\left(x,0\right)$. Clearly, for any $y\in \mathbb{R}$, we have $f\left(x,y\right) = f\left(x,0\right) = g\left(x\right)$, where the first equality follows from the independence of the second variable of $f$.

For the other direction, assume there is a $g$ such that $f\left(x,y\right)=g\left(x\right)$ for all $x,y \in \mathbb{R}$. Then let $y_1, y_2 \in \mathbb{R}$, arbitrary. Then we have
$$ f\left(x,y_1\right) = g\left(x\right) = f\left(x,y_2\right) $$ and so $f$ is independent of the second variable.

We wish to determine the derivative of such an $f$ in terms of the univariate function $g$. Our experience with partial derivatives from calculus 3 tells us that we should expect the derivative with respect to the second variable is zero, since the function doesn't change as we vary it. Also, the derivative with respect to the first variable is likely the derivative of the univariate function. In all, we guess that
$$ f'\left(x,y\right) = \left[g'\left(x\right),0\right] $$ Let us use this in the definition of derivative to verify that we obtain a zero limit:
\begin{equation*} \begin{split}
\lim_{\left(h,k\right)\rightarrow \left(0,0\right)} \frac{\left| f\left(x+h,y+k\right)-f\left(x,y\right)-f'\left(x,y\right) \left(h,k\right)\right|}{\left| \left(h,k\right)\right|} & = \lim_{\left(h,k\right)\rightarrow \left(0,0\right)} \frac{\left| g\left(x+h\right) - g\left(x\right) - g'\left(x\right) \left(h\right)\right|}{\left|\left(h,k\right)\right|} \\
& \leq \lim_{\left(h,k\right)\rightarrow \left(0,0\right)} \frac{\left| g\left(x+h\right) - g\left(x\right) - g'\left(x\right) \left(h\right)\right|}{\left|h\right|} \\
& = 0 \\
\end{split}\end{equation*}
where the last equality holds if $g$ is differentiable at $a$. Thus, the original difference quotient must be zero, confirming that the derivative of $f$ at $\left(a,b\right)^T$ is, indeed, $\left(g'\left(a\right),0\right)^T$.
\end{proof}




\begin{problem}{2.3}
Define when a function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ is independent of the first variable and find $f'\left(a,b\right)$ for such $f$. Which functions are independent of the first variable and also of the second variable?
\end{problem}

\begin{proof}
This is quite obvious with some calculus 3 intuition. A function $f$ is independent of the first variable if $f\left(x_1,y\right) = f\left(x_2,y\right)$ for any $x_1, x_2, y \in \mathbb{R}$. From the previous problem, we are sure that there exists some function $g:\mathbb{R}\rightarrow \mathbb{R}$ such that $f\left(x,y\right) = g\left(y\right)$ for any $x,y\in \mathbb{R}$. Similar to the previous problem, again, we know that the derivative of such an $f$ is $f'\left(a,b\right) = \left(0,g'\left(b\right)\right)^T$.

If a function of two variables is independent of both variables, then let $z = f\left(0,0\right)$. Then for any $\left(x,y\right) \in \mathbb{R}^2$, we have
$$ f\left(x,y\right) = f\left(0,y\right) = f\left(0,0\right) = z$$ and so $f\left(x,y\right) = z$ for any $x,y\in \mathbb{R}$, hence $f$ is a constant function. Conversely, if $f$ is a constant function then it is trivially independent of both variables. Thus, the constant functions are exactly those which are independent of both variables.
\end{proof}







\begin{problem}{2.4}
Let $g$ be a continuous real-valued function on the unit circle $\left\{ x\in \mathbb{R}^2: \; \left|x\right| = 1\right\}$ such that $g\left(0,1\right) = g\left(1,0\right)=0$ and $g\left(-x\right) = -g\left(x\right)$. Define $f:\mathbb{R}^2\rightarrow \mathbb{R}$ by
\begin{equation*}
f\left(x\right) = \left\{ \begin{array}{lr} \left|x\right|\cdot g\left(\frac{x}{\left|x\right|}\right) & x\neq 0 \\ 0 & x=0 \end{array} \right.
\end{equation*}
\begin{itemize}
	\item If $x\in \mathbb{R}^2$ and $h:\mathbb{R}\rightarrow \mathbb{R}$ is defined by $h\left(t\right) = f\left(tx\right)$, show that $h$ is differentiable. \\
	\item Show that $f$ is not differentiable at $\left(0,0\right)$ unless $g=0$. \textit{Hint}: First show that $Df\left(0,0\right)$ would have to be $0$ by considering $\left(h,k\right)$ with $k=0$ and then with $h=0$.
\end{itemize}
\end{problem}

\begin{proof}
\begin{itemize}
	\item Let $h\left(t\right) = f\left(tx\right)$ for some fixed $x\in \mathbb{R}^2$. Consider first the case where $x=0$. Then 
	$$ h\left(t\right) = f\left(0\right) = 0 $$ everywhere, and so $h$ is trivially differentiable since it is constant, and $h' = 0$.
	
	Consider now $x \neq 0$. If $t>0$ then 
	$$ h\left(t\right) = \left|tx\right| \cdot g\left( \frac{tx}{\left|tx\right|} \right)  = tx \cdot g\left( \frac{x}{\left|x\right|}\right) = t f\left(x\right) $$
	Alternately, if $x \neq 0$ and $t<0$, then
	$$ h\left(t\right) = \left|tx\right| \cdot g\left( \frac{tx}{\left|tx\right|} \right)  = -tx \cdot g\left( -\frac{x}{\left|x\right|}\right) = tx \cdot g\left( \frac{x}{\left|x\right|}\right) = t f\left(x\right) $$
	Lastly, if $x \neq 0$ and $t =0 $, then
	$$ h\left(t\right) = h\left(0\right) = f\left(0\right) = 0 = 0 \cdot f\left(x\right) $$
	Thus, in the case $x\neq 0$, we have shown by exhaustion that $h\left(t\right) = f\left(tx\right) = t f\left(x\right) $ for all $t \in \mathbb{R}$. This is a simple linear function and is differentiable with derivative $h'\left(t\right) = f\left(x\right)$.
	
	\item Assume $f$ is differentiable at $\left(0,0\right)$ with $g \neq 0$. As the hint suggests, we consider taking the derivative along two different paths and show that the value of the derivative is not unique, a contradiction to the fact that differentiable functions have unique derivatives.
	
First, consider $k=0$. Then the difference quotient is
\begin{equation*}\begin{split}
0 & = \lim_{\left(h,0\right)\rightarrow \left(0,0\right)} \frac{\left| f\left(h,0\right) - f\left(0,0\right) - Df\left(0,0\right) \cdot \left(h,0\right)\right|}{\left| \left(h,0\right)\right|} \\
& = \lim_{h\rightarrow 0} \frac{\left| \left|\left(h,0\right)\right| g\left( \frac{\left(h,0\right)}{\left|\left(h,0\right)\right|}\right) - Df\left(0,0\right) \cdot \left(h,0\right)\right|}{\left| h\right|} \\
& = \lim_{h\rightarrow 0} \frac{\left| \left|h \right| g\left( \frac{h}{\left|h\right|}, 0\right) - Df\left(0,0\right) \cdot \left(h,0\right)\right|}{\left| h\right|} \\
& = \lim_{h\rightarrow 0} \frac{\left| \left|h \right| g\left( \pm 1, 0\right) - Df\left(0,0\right) \cdot \left(h,0\right)\right|}{\left| h\right|} \\
& = \lim_{h\rightarrow 0} \frac{\left| Df\left(0,0\right) \cdot \left(h,0\right)\right|}{\left| h\right|} \\
\end{split}\end{equation*}
Recall that the linear map $Df\left(0,0\right): \mathbb{R}^2 \rightarrow \mathbb{R}$ can be represented as a $1\times 2$ array, say $Df\left(0,0\right) = \left[a,b\right]$. Then this last expression becomes
\begin{equation*}\begin{split}
0 & = \lim_{h\rightarrow 0} \frac{\left| ah\right|}{\left| h\right|} =  \lim_{h\rightarrow 0} \frac{\left|a\right|\left|h\right|}{\left|h\right|} = \left| a\right| \\
\end{split}\end{equation*}
Hence $\left|a\right| = 0$. Similarly, we can show that $\left|b\right| = 0$ by the same procedure, but letting $h=0$. Hence, the derivative is $Df\left(0,0\right) = \left[0,0\right]$, and so $Df\left(0,0\right)\left(h,k\right) = 0$ for any $\left(h,k\right)\in \mathbb{R}^2$. In the difference quotient, we have
\begin{equation*}\begin{split}
0 & = \lim_{\left(h,k\right)\rightarrow \left(0,0\right)} \frac{\left| f\left(h,k\right) - f\left(0,0\right) - Df\left(0,0\right) \cdot \left(h,k\right)\right|}{\left| \left(h,k\right)\right|} \\
& = \lim_{\left(h,k\right)\rightarrow \left(0,0\right)} \frac{\left| f\left(h,k\right) \right|}{\left| \left(h,k\right)\right|} \\
& = \lim_{\left(h,k\right)\rightarrow \left(0,0\right)} \frac{\left| \left|\left(h,k\right)\right| \cdot g\left( \frac{\left(h,k\right)}{\left| \left(h,k\right)\right|}\right) \right|}{\left| \left(h,k\right)\right|} \\
& = \lim_{\left(h,k\right)\rightarrow \left(0,0\right)} \left|  g\left( \frac{\left(h,k\right)}{\left| \left(h,k\right) \right|} \right) \right| \\
& \neq 0
\end{split}\end{equation*}
since we assumed $g\neq 0$. This is quite clearly a contradiction, and so our assumption that $f$ is differentiable at $\left(0,0\right)$ with $g\neq 0$ is incorrect. If $g=0$, then $f$ is certainly differentiable at $\left(0,0\right)$.

\end{itemize}
\end{proof}









\begin{problem}{2.5}
Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ be defined by
\begin{equation*}
f\left(x,y\right) = \left\{ \begin{array}{lr} \frac{x\left|y\right|}{\sqrt{x^2+y^2}} & \left(x,y\right) \neq 0 \\ 0 & \left(x,y\right) =0 \end{array} \right.
\end{equation*}
Show that $f$ is a function of the kind considered in Problem 2-4 so that $f$ is not differentiable at $\left(0,0\right)$.
\end{problem}







\begin{problem}{2.6}
Let $f:\mathbb{R}^2\rightarrow \mathbb{R}$ be defined by $f\left(x,y\right) = \sqrt{\left|xy\right|}$. Show that $f$ is not differentiable at $\left(0,0\right)$.
\end{problem}

\begin{proof}
As in Problem 2-4, we will assume $f$ is differentiable at $\left(0,0\right)$ and consider different paths toward $\left(0,0\right)$ until we hit a contradiction. Note that $f\left(0,0\right)=0$ and that we can, as usual, express the derivative as $Df\left(0,0\right) = \left[a,b\right]$. By our assumption of differentiability, we have
$$ 0 = \lim_{\left(h,k\right)\rightarrow \left(0,0\right)} \frac{\left|f\left(h,k\right) - f\left(0,0\right) - Df\left(0,0\right) \left(h,k\right)\right|}{\left|\left(h,k\right)\right|} $$ 

First consider $k=0$. Then this difference quotient becomes
\begin{equation*}\begin{split} 
0 & = \lim_{h\rightarrow 0} \frac{\left| f\left(h,0\right) - Df\left(0,0\right)\left(h,0\right)\right|}{\left|h\right|} \\
& = \lim_{h\rightarrow 0} \frac{\left|a h\right|}{\left|h\right|} \\
& = \left|a\right| \\
\end{split}\end{equation*}
and so $a= 0$. By the symmetry of the function $f$, we can repeat this to show that $b=0$. That is, the derivative at $0$ must be $Df\left(0,0\right) = \left[0,0\right]$. So far, so good; there is no contradiction.

Next, let us try $h=k=u$ and let $u\rightarrow 0$. The derivative difference quotient becomes
\begin{equation*}\begin{split}
0 & = \lim_{u\rightarrow 0} \frac{\left| f\left(u,u\right) - f\left(0,0\right) - Df\left(0,0\right) \left(u,u\right)\right|}{\left|\left(u,u\right)\right|} \\
& = \lim_{u\rightarrow 0} \frac{\left| \sqrt{\left|u^2\right|} \right|}{\sqrt{2 \left|u\right|^2}} \\
& = \frac{1}{\sqrt{2}} \lim_{u\rightarrow 0} \frac{\left| u\right|}{\left| u\right|} \\
& = 2^{-1/2} \\
\end{split}\end{equation*}
which is certainly not zero. Hence, we have a contradiction, and so we must conclude that $f$ is not differentiable at $\left(0,0\right)$.
\end{proof}





\begin{problem}{2.7}
Let $f:\mathbb{R}^n\rightarrow \mathbb{R}$ be a function such that $\left|f\left(x\right)\right| \leq \left|x\right|^2$. Show that $f$ is differentiable at $0$.
\end{problem}

\begin{proof}
Let $f$ be such a function. Let's think about a special case to try to get some intuition. If we consider $n=1$, we have $f:\mathbb{R}\rightarrow \mathbb{R}$ and $\left|f\left(x\right)\right| \leq \left|x\right|^2$. If we picture the graph of this function, the function will be bounded within the curves $y=x^2$ and $y=-x^2$. As we move closer to the origin, the function $f$ gets ``pinched'' by these bounds; further, the derivatives of these bounding curves at the origin are both $0$, and so it is reasonable to guess that $f'\left(0\right)=0$. Generalizing to arbitrary $n$, we will guess that $Df\left(0\right) = 0 \in \mathbb{R}^{1 \times n}$.

Let us use the definition and the condition on $f$ to prove this. The difference quotient is the quantity
\begin{equation*}\begin{split}
DQ = \frac{\left|f\left(0+h\right) - f\left(0\right) - Df\left(0\right) \left(h\right)  \right|}{\left| h\right|}
\end{split}\end{equation*}

First, note that $\left|f\left(x\right)\right| \leq \left| x\right|^2$ means that $ \left|f\left(0\right)\right| \leq 0$ so that $f\left(0\right)=0$. Then, with our guess that $Df\left(0\right) = 0\in \mathbb{R}^{1\times n}$, we have
\begin{equation*}
DQ = \frac{\left| f\left(h\right) \right|}{\left| h\right|} \leq \frac{\left|h\right|^2}{\left|h\right|} = \left|h\right|
\end{equation*}
Finally, taking the limit of the difference quotient, we obtain
\begin{equation*}
\lim_{h\rightarrow 0} DQ \leq \lim_{h\rightarrow 0} \left|h\right| = 0
\end{equation*}
Thus, by definition of derivative, $f$ is differentiable at $0\in \mathbb{R}^n$, and the derivative is $Df\left(0\right) = 0 \in \mathbb{R}^{1\times n}$.
\end{proof}




\begin{problem}{2.8}
Let $f:\mathbb{R}\rightarrow\mathbb{R}^2$. Prove that $f$ is differentiable at $a \in \mathbb{R}$ if and only if $f^1$ and $f^2$ are, and that in this case
$$ f'\left(a\right) = \left( \begin{array}{c} \left(f^1\right)'\left(a\right) \\ \left(f^2 \right)'\left(a\right) \end{array} \right)$$
\end{problem}

\begin{proof}
This problem hints very strongly toward the general structure of the derivative (Jacobian) of a multivariable, multivalued function by considering a simple case. 

First, assume the function $f$ is differentiable at $a\in\mathbb{R}$. Then in a sense we can decompose the difference quotient involving $f$ into two similar difference quotients involving the component functions $f^1$ and $f^2$:
\begin{equation*}\begin{split}
\frac{\left| f\left(a+h\right)  - f\left(a\right) - Df\left(a\right)\left(h\right)\right|^2}{\left| h\right|^2} =  & \frac{\left| f^1\left(a+h\right) - f^1\left(a\right) - \left(Df\left(a\right)\right)^1\left(h\right)\right|^2}{\left| h\right|^2}  \\ & + \frac{\left| f^2\left(a+h\right) - f^2\left(a\right) - \left(Df\left(a\right)\right)^2\left(h\right)\right|^2}{\left| h\right|^2}
\end{split}\end{equation*}
where the superscripts on the right-hand side indicate component functions and elements of the derivative. Since the left hand side is equal to a sum of squares, the left-hand side is greater than either of the right-hand sides. Taking the square root of both sides and then taking the limit as $h\rightarrow 0$ gives the desired results; for instance,
$$ 0 = \lim_{h\rightarrow 0} \frac{\left| f\left(a+h\right)  - f\left(a\right) - Df\left(a\right)\left(h\right)\right|^2}{\left| h\right|^2} \geq \lim_{h\rightarrow 0} \frac{\left| f^1\left(a+h\right) - f^1\left(a\right) - \left(Df\left(a\right)\right)^1\left(h\right)\right|^2}{\left| h\right|^2}  $$ and so the derivative of $f^1$ is the first component of the derivative of $f$, namely $\left(Df\left(a\right)\right)^1$. Repeating this process for the other term on the right-hand side, we have the derivative of $f^2$ is $\left(Df\left(a\right)\right)^2$. Thus, the component function are differentiable, and we have 
$$ f'\left(a\right) = Df\left(a\right) = \left( \begin{array}{c} \left(f^1\right)'\left(a\right) \\ \left(f^2\right)'\left(a\right) \end{array}\right) $$

Conversely, assume that the component functions are differentiable. Then we can use the triangle inequality on the difference quotient for $f$, similar to what we did above (the difference is that above we actually squared out the components exactly, whereas here we can use the triangle inequality on the linear combination $f = f^1 e_1 + f^2 e_2$ to get $\left|f\right| \leq \left|f^1\right| + \left|f^2\right|$).
\begin{equation*}\begin{split}
\frac{\left| f\left(a+h\right)  - f\left(a\right) - \left[\left(f^1\right)' \left(a\right), \left(f^2\right)'\left(a\right) \right]\left(h\right)\right|}{\left| h\right|} \leq  & \frac{\left| f^1\left(a+h\right) - f^1\left(a\right) - \left(f^1\left(a\right)\right)'\left(h\right)\right|}{\left| h\right|}  \\ & + \frac{\left| f^2\left(a+h\right) - f^2\left(a\right) - \left(f^2\right)'\left(a\right)\left(h\right)\right|}{\left| h\right|}
\end{split}\end{equation*}
Taking the limit of both sides as $h\rightarrow 0$, the right-hand side goes to zero since we assumed $f^1$ and $f^2$ are differentiable at $a$. Thus, the limit of the left-hand side is zero, and this is the definition of the differentiability of $f$ at $a$. Also, the derivative we used was exactly the one identified in the other direction of this proof.
\end{proof}





\begin{problem}{2.9}
Two functions $f,g:\mathbb{R}\rightarrow \mathbb{R}$ are equal up to \textit{n}th order at $a$ if 
$$ \lim_{h\rightarrow 0} \frac{f\left(a+h\right) -g\left(a+h\right)}{h^n} = 0 $$
\begin{itemize}
	\item Show that $f$ is differentiable at $a$ if and only if there is a function $g$ of the form $g\left(x\right) = a_0 + a_1 \left(x-a\right)$ such that $f$ and $g$ are equal up to first order at $a$. \\
	\item If $f'\left(a\right),\ldots,f^{\left(n\right)}\left(a\right)$ exist, show that $f$ and the function $g$ defined by
	$$ g\left(x\right) = \sum_{i=0}^n \frac{f^{\left(i\right)}\left(a\right)}{i!} \left(x-a\right)^i $$ are equal up the \textit{n}th order at $a$. \textit{Hint}: the limit 
	$$ \lim_{x\rightarrow a} \frac{f\left(x\right) - \sum_{i=0}^{n-1} \frac{f^{\left(i\right)}\left(a\right)}{i!} \left(x-a\right)^i}{\left(x-a\right)^n} $$ may be evaluated by L'Hospital's rule.
\end{itemize} 
\end{problem}

\begin{proof}
This problem makes concrete the notion of the derivative as a low-order approximation to the function.
\begin{itemize}
	\item Assume $f$ is differentiable at $a$. Note the similarity of the defintion of ``equal up to \textit{n}th order'' to the difference quotient in the derivative. This is highly suggestive, and after playing around a little bit we can guess the function $g\left(x\right) = f\left(a\right) + Df\left(a\right) \left(x-a\right)$, i.e., $a_0 = f\left(a\right)$ and $a_1 = Df\left(a\right)$. Then by the definition of differentiability of $f$ at $a$, we have
	\begin{equation*}
	\begin{split}	
	0 & = \lim_{h\rightarrow 0} \frac{\left| f\left(a+h\right)-f\left(a\right) - Df\left(a\right) h \right|}{\left|h\right|} \\ 
	& = \lim_{h\rightarrow 0} \frac{\left| f\left(a+h\right) - g\left(a+h\right) \right|}{\left|h\right|}
	\end{split}
	\end{equation*}
Thus $f$ and $g$ are equal up to order $1$.

Conversely, let there exist some $g\left(x\right) = a_0 + a_1 \left(x-a\right)$ which is equal to $f$ up to order 1. First, we want to show that the terminology ``up to order $n$'' is suggestive in that if two functions are equal up to order $n$, then they are equal up to order $k$ for all $k = 0, 1,\ldots, n$. Assume $f$ and $g$ are equal up to order $n$. Then
\begin{equation*}\begin{split} \lim_{h\rightarrow 0} \frac{f\left(a+h\right)-g\left(a+h\right)}{h^{n-1}} & = \lim_{h\rightarrow 0} \frac{f\left(a+h\right)-g\left(a+h\right)}{h^n} h^n \\
& = \left( \lim_{h\rightarrow 0} \frac{f\left(a+h\right) - g\left(a+h\right)}{h^n} \right) \left( \lim_{h\rightarrow 0} h^n \right) \\
& = 0 \cdot 0 \\
& = 0
\end{split}\end{equation*}
Hence $f$ and $g$ are equal up to order $n-1$ if they are equal up to order $n$. We could continue for all orders under $n$, but for our problem we don't need this.

We can start the converse now. Assume $f$ and $g$ are equal up to order 1, and $g\left(x\right) = a_0 +a_1\left(x-a\right)$. Then by the previous paragraph they are equal up to order zero, i.e.,
\begin{equation*}\begin{split}
0 & =  \lim_{h\rightarrow 0} \frac{f\left(a+h\right)-g\left(a+h\right)}{h^0} \\ & = \lim_{h\rightarrow 0} f\left(a+h\right) - \lim_{h\rightarrow 0} g\left(a+h\right) \\
& = f\left(a\right) - a_0
\end{split}\end{equation*}
where we used the fact that differentiability of $f$ at $a$ guarantees continuity of $f$ at $a$, and we used the particular form (and continuity) of $g$. Thus, $f\left(a\right) = a_0$, as we suspected from the previous problem. 

Lastly, since $f$ and $g$ are equal up to order 1, we have
\begin{equation*}\begin{split}
0 & = \lim_{h\rightarrow 0} \frac{f\left(a+h\right) - g\left(a+h\right)}{h^1} \\
& = \lim_{h\rightarrow 0} \frac{f\left(a+h\right)-f\left(a\right) - a_1 h}{h}
\end{split}\end{equation*}
This last equation says that $f$ is differentiable at $a$, with derivative $f'\left(a\right) = a_1$. \\

	\item Assume that the first $n$ derivatives of $f$ at $a$ exist, and let $g$ be defined as in the hypothesis. To show that $f$ and $g$ are equal up to order $n$, we look at
	\begin{equation*}\begin{split}
	L = \lim_{h\rightarrow 0} \frac{f\left(a+h\right)-g\left(a+h\right)}{h^n} & = \lim_{h\rightarrow 0} \frac{f\left(a+h\right) - \sum_{i=0}^n \frac{f^{\left(i\right)} \left(a\right)}{i!} h^i } {h^n} \\
	& = \lim_{x\rightarrow a} \frac{f\left(x\right) - \sum_{i=0}^n \frac{f^{\left(i\right)} \left(a\right)}{i!} \left(x-a\right)^i}{\left(x-a\right)^n}
	\end{split}\end{equation*}
where we made a change of variables $x=a+h$. Plugging $a$ into this quotient, we obtain the indeterminate form $0/0$, hence L'Hopital's rule applies:
\begin{equation*}\begin{split}
L & = \lim_{x\rightarrow a} \frac{f'\left(x\right) - \sum_{i=1}^n \frac{f^{\left(i\right)}}{i!} i\left(x-a\right)^{\left(i-1\right)}}{n \left(x-a\right)^{\left(n-1\right)}}
\end{split}\end{equation*}
However, if we evaluate this quotient at $x=a$, we again obtain the same indeterminate form, $0/0$. We continue this process of applying L'Hopital's rule, i.e., we repeatedly differentiate the numerator and denominator:
\begin{equation*}\begin{split}
L & = \lim_{x\rightarrow a} \frac{f^{\left(2\right)}\left(x\right) - \sum_{i=2}^n \frac{f^{\left(i\right)}}{i!} i\left(i-1\right) \left(x-a\right)^{\left(i-2\right)} }{n\left(n-1\right) \left(x-a\right)^{\left(n-2\right)}} \\
& = \cdots \\
& = \lim_{x\rightarrow a} \frac{f^{\left(n-1\right)}\left(x\right) - \sum_{i=n-1}^n \frac{f^{\left(i\right)}}{i!} i\left(i-1\right)\left(i-2\right) \cdots \left(i-\left(n-2\right)\right) \left(x-a\right)^{\left(i-\left(n-1\right)\right)} }{n\left(n-1\right)\left(n-2\right) \cdots \left(n-\left(n-1\right)\right) \left(x-a\right)^{\left(n-\left(n-2\right)\right)}} \\
& = \lim_{x\rightarrow a} \frac{f^{\left(n\right)}\left(x\right) - \sum_{i=n}^n \frac{f^{\left(i\right)}}{i!} i\left(i-1\right)\left(i-2\right) \cdots \left(i-\left(n-1\right)\right) \left(x-a\right)^{\left(i-n\right)} }{n\left(n-1\right)\left(n-2\right) \cdots \left(n-\left(n-1\right)\right) \left(x-a\right)^{\left(n-n\right)}}
\end{split}\end{equation*}
Notice that this last equation simplifies quite nicely:
\begin{equation*}\begin{split}
L & = \lim_{x\rightarrow a} \frac{f^{\left(n\right)}\left(x\right) - \frac{f^{\left(n\right)}\left(a\right)}{n!} n! \left(x-a\right)^0}{n! \left(x-a\right)^0} \\
& = \frac{1}{n!} \lim_{x\rightarrow a}  \left[ f^{\left(n\right)}\left(x\right) - f^{\left(n\right)} \left(a\right) \right] \\
& = \frac{1}{n!} \left[ f^{\left(n\right)} \left(a\right) - f^{\left(n\right)} \left(a\right)\right] \\
& = 0
\end{split}\end{equation*}
where the limit is able to be taken because since $f^{\left(n\right)}$ is differentiable, it is continuous.\footnote{This is a problem, since we only have guaranteed the first $n$ derivatives to exist, so we cannot be sure that $f^{\left(n\right) }$ itself is differentiable. We will need to take a closer look at this.} Hence, $L =0$, which is what we needed in order to say that $f$ and $g$ are equal up to order $n$.
\end{itemize}
\end{proof}







\begin{problem}{2.10}
Use the theorems of this section to find $f'$ for the following:
\begin{itemize}
	\item $f\left(x,y,z\right) = x^y $
	\item $f\left(x,y,z\right) = \left(x^y, z\right)$
	\item $f\left(x,y\right) = \sin \left(x\sin y\right)$
	\item $f\left(x,y,z\right) = \sin \left( x\sin \left(y\sin z\right) \right)$
	\item $f\left(x,y,z\right) = x^{y^z} $
	\item $f\left(x,y,z\right) = x^{y+z} $
	\item $f\left(x,y,z\right) = \left(x+y\right)^z $
	\item $f\left(x,y\right) = \sin\left(xy\right) $
	\item $f\left(x,y\right) = \left[\sin \left(xy\right) \right]^{\cos 3} $
	\item $f\left(x,y\right) = \left( \sin \left(xy\right), \sin\left(x \sin y\right), x^y \right)$
\end{itemize}
\end{problem}






\begin{problem}{2.11}
Find $f'$ for the following (where $g:\mathbb{R}\rightarrow \mathbb{R}$ is continuous):
\begin{itemize}
	\item $f\left(x,y\right) = \int_a^{x+y} g$ 
	\item $f\left(x,y\right) = \int_a^{x\cdot y} g$
	\item $f\left(x,y,z\right) = \int_{x^y}^{\sin \left(x \sin \left(y \sin z\right)\right)} g $
\end{itemize}
\end{problem}






\begin{problem}{2.12}
A function $f: \mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R}^p$ is bilinear if for $x,x_1,x_2 \in \mathbb{R}^n$, $y,y_1,y_2 \in \mathbb{R}^m$, and $a\in \mathbb{R}$ we have
\begin{equation*}
\begin{split}
f\left(ax,y\right) & = af\left(x,y\right) = f\left(x,ay\right) \\
f\left(x_1+x_2,y\right) & = f\left(x_1,y\right) + f\left(x_2,y\right) \\
f\left(x,y_1+y_2\right) & = f\left(x,y_1\right) + f\left(x,y_2\right) \\
\end{split}
\end{equation*}
\begin{itemize}
	\item Prove that if $f$ is bilinear, then
	$$ \lim_{\left(h,k\right)\rightarrow 0} \frac{\left|f\left(h,k\right)\right|}{\left| \left(h,k\right)\right|} = 0$$
	\item Prove that $Df\left(a,b\right)\left(x,y\right) = f\left(a,y\right) + f\left(x,b\right) $ 
	\item Show that the formula for $Dp\left(a,b\right)$ in Theorem 2-3 is a special case of (b).\\
\end{itemize}
\end{problem}


\begin{proof}
\begin{itemize}
	\item Assume $f:\mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R}^p$ is bilinear. 
\end{itemize}
\end{proof}








\begin{problem}{2.13} 
Define $IP: \mathbb{R}^n\times \mathbb{R}^n \rightarrow \mathbb{R}$ by $IP\left(x,y\right) = \left\langle x,y\right\rangle$.
\begin{itemize}
	\item Find $D\left(IP\right)\left(a,b\right)$ and $\left(IP\right)'\left(a,b\right)$. 
	\item If $f,g:\mathbb{R}\rightarrow\mathbb{R}^n$ are differentiable and $h:\mathbb{R}\rightarrow \mathbb{R}$ is defined by $h\left(t\right) = \left\langle f\left(t\right),g\left(t\right)\right\rangle$, show that
	$$ h'\left(a\right) = \left\langle f'\left(a\right)^T, g\left(a\right) \right\rangle + \left\langle f\left(a\right), g'\left(a\right)^T \right\rangle $$ (Note that $f'\left(a\right)$ is an $n\times 1$ matrix; its transpose $f'\left(a\right)^T$ is a $1 \times n$ matrix, which we consider as a member of $\mathbb{R}^n$.)
	\item If $f:\mathbb{R}\rightarrow \mathbb{R}^n$ is differentiable and $\left|f\left(t\right)\right| = 1$ for all $t$, show that $\left\langle f'\left(t\right)^T, f\left(t\right)\right\rangle = 0$.
	\item Exhibit a differentiable function $f:\mathbb{R}\rightarrow \mathbb{R}$ such that the function $\left|f\right|$ defined by $\left|f\right|\left(t\right) = \left|f\left(t\right)\right|$ is not differentiable.
\end{itemize}
\end{problem}













\begin{problem}{2.14}
Let $E_i$, $i=1,\ldots,k$ be Euclidean spaces of various dimensions. A function $f: E_1 \times \cdots \times E_k \rightarrow \mathbb{R}^p$ is called multilinear if for each choice of $x_j\in E_j$, $j\neq i$ the function $g:E_i\rightarrow \mathbb{R}^p$ defined by $g\left(x\right) = f\left(x_1,\ldots,x_{i-1},x,x_{i+1},\ldots,x_k\right)$ is a linear transformation.
\begin{itemize}
	\item If $f$ is multilinear and $i\neq j$, show that for $h=\left(h_1,\ldots,h_k\right)$, with $h_l \in E_l$, we have
	$$ \lim_{h\rightarrow 0} \frac{\left| f\left(a_1,\ldots,h_i,\ldots, h_j,\ldots,a_k\right)\right|}{\left|h\right|} = 0$$ \textit{Hint}: if $g\left(x,y\right)=f\left(a_1,\ldots,x,\ldots,y,\ldots,a_k\right)$, then $g$ is bilinear. 
	\item Prove that 
	$$ Df\left(a_1,\ldots,a_k\right)\left(x_1,\ldots,x_k\right) = \sum_{i=1}^k f\left(a_1,\ldots, a_{i-1},x_i,a_{i+1},\ldots,a_k\right)$$
\end{itemize}
\end{problem}









\begin{problem}{2.15}
Regard an $n\times n$ matrix as a point in the $n$-fold product $\mathbb{R}^n \times \cdots \times \mathbb{R}^n$ by considering each row as a member of $\mathbb{R}^n$.
\begin{itemize}
	\item Prove that $\text{det}:\mathbb{R}^n \times \cdots \times \mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable and
	$$ D\left(\text{det}\right) \left(a_1,\ldots,a_n\right)\left(x_1,\ldots,x_n\right) = \sum_{i=1}^n \text{det} \left( \begin{array}{c} a_1 \\ \vdots \\ x_i \\ \vdots \\ a_n \end{array} \right) $$
	\item If $a_{ij}:\mathbb{R}\rightarrow \mathbb{R}$ are differentiable and $f\left(t\right) = \text{det}\left(a_{ij}\left(t\right)\right)$, show that
	$$ f'\left(t\right) = \sum_{j=1}^n \text{det} \left( \begin{array}{ccc} a_{11}\left(t\right) & \cdots & a_{1n}\left(t\right) \\
	\vdots & & \vdots  \\
	a_{j1}'\left(t\right) & \cdots & a_{jn}'\left(t\right) \\
	\vdots & & \vdots \\
	a_{n1}\left(t\right) & \cdots & a_{nn}\left(t\right)
	\end{array}\right) $$
	\item If $\text{det}\left(a_{ij}\left(t\right)\right) \neq 0$ for all $t$ and $b_1,\ldots,b_n:\mathbb{R}\rightarrow \mathbb{R}$ are differentiable, let $s_1,\ldots,s_n:\mathbb{R}\rightarrow \mathbb{R}$ be the functions such that $s_1\left(t\right),\ldots,s_n\left(t\right)$ are the solutions of the equations
	$$ \sum_{j=1}^n a_{ji}\left(t\right)s_j\left(t\right) = b_i\left(t\right) \hspace{10mm} i=1,\ldots,n$$ Show that $s_i$ is differentiable and find $s_i'\left(t\right)$.
\end{itemize}
\end{problem}


\begin{problem}{2.16}
Suppose $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ is differentiable and has a differentible $f^{-1}: \mathbb{R}^n \rightarrow \mathbb{R}^n$. Show that $\left(f^{-1}\right)'\left(a\right) = \left[f'\left(f^{-1}\left(a\right)\right)\right]^{-1}$. \textit{Hint}: $f \circ f^{-1}\left(x\right) = x$. 
\end{problem}




\begin{problem}{2.17}
Find the partial derivatives of the following functions:
\begin{itemize}
	\item $f\left(x,y,z\right) = x^y$
	\item $f\left(x,y,z\right) = z$
	\item $f\left(x,y\right) = \sin \left(x \sin y\right)$
	\item $f\left(x,y,z\right) = \sin \left( x \sin \left(y \sin z\right)\right)$
	\item $f\left(x,y,z\right) = x^{y^z}$
	\item $f\left(x,y,z\right) = x^{y+z} $
	\item $f\left(x,y,z\right) = \left(x+y\right)^z$
	\item $f\left(x,y\right) = \sin\left(xy\right)$
	\item $f\left(x,y\right) = \left[\sin \left(xy\right)\right]^{\cos 3} $
\end{itemize}
\end{problem}
\begin{proof}
Note: we do not need to use the limit definition of the partial derivative because for each derivative we can treat the multivariate functions as functions of only the variable we are differentiating with respect to. \\
\begin{itemize}
	\item $ D_1 f\left(x,y,z\right) = y x^{y-1}$ \\ $D_2 f\left(x,y,z\right) = \log\left(x\right) x^y$ \\$D_3 f\left(x,y,z\right) = 0$
	\item $D_1 f\left(x,y,z\right) = D_2 f\left(x,y,z\right) = 0$ \\ $D_3 
f\left(x,y,z\right) = 1$
	\item $D_1 f\left(x,y,z\right) = \sin \left(y\right) \cos\left(x\sin \left(y\right)\right)$ \\ $D_2f\left(x,y,z\right) = x\cos\left(y\right) \cos\left(x\sin\left(y\right)\right)$ \\ $D_3 f\left(x,y,z\right)=0$
	\item $D_1 f\left(x,y,z\right) = \sin\left(y \sin\left(z\right)\right) \cos\left(x\sin \left(y\sin \left(z\right)\right)\right) $ \\ $D_2 f\left(x,y,z\right) = \sin\left(z\right) x\cos \left(y\sin\left(z\right)\right) \cos \left(x\sin \left(y\sin\left(z\right)\right)\right)$ \\ $D_3 f\left(x,y,z\right) = y\cos \left(z\right) x\sin \left(y\sin\left(z\right)\right) \cos\left(x\sin\left(y\sin\left(z\right)\right)\right) $
	\item $D_1 f\left(x,y,z\right) = y^z x^{y^z-1}$\\ $D_2 f\left(x,y,z\right) = zy^{z-1} \log\left(x\right) x^{y^z}$\\ $D_3 f\left(x,y,z\right) = \log\left(x\right) \log\left(y\right) y^z x^{y^z}$
	\item $D_1 f\left(x,y,z\right) = \left(y+z\right) x^{y+z-1}$\\ $D_2 f\left(x,y,z\right) = \log\left(x\right) x^{y+z}$ \\ $D_3 f\left(x,y,z\right) = \log\left(x\right) x^{y+z} $
	\item $D_1 f\left(x,y,z\right) = z \left(x+y\right)^{z-1}$\\ $D_2 f\left(x,y,z\right) = z\left(x+y\right)^{z-1}$ \\ $D_3 f\left(x,y,z\right) = \log\left(x+y\right) \left(x+y\right)^z$
	\item $D_1 f\left(x,y\right) = y\cos\left(xy\right)$ \\ $D_2 f\left(x,y\right) = x\cos\left(xy\right)$
	\item $D_1 f\left(x,y\right) = \cos \left(3\right) \sin\left(xy\right)^{\cos\left(3\right)-1} y\cos\left(xy\right)$ \\ $D_2 f\left(x,y\right) = \cos \left(3\right) \sin\left(xy\right)^{\cos\left(3\right)-1} x\cos\left(xy\right)$
\end{itemize}
\end{proof}





\begin{problem}{2.18}
Find the partial derivatives of the following functions (where $g:\mathbb{R}\rightarrow \mathbb{R}$ is continuous):
\begin{itemize}
	\item $f\left(x,y\right) = \int_a^{x+y} g$
	\item $f\left(x,y\right) = \int_y^x g$
	\item $f\left(x,y\right) = \int_a^{xy} g$
	\item $f\left(x,y\right) = \int_a^{\left( \int_b^y g\right)} g$
\end{itemize}
\end{problem}
Recall that the Second Fundamental Theorem of Calculus states that for continuous functions $g$, for $f$ defined as $$ f\left(x\right) = \int_a^x g\left(t\right)\, dt $$ we have $$ f'\left(x\right) = g\left(x\right)$$.\\

\begin{proof}
\begin{itemize}
	\item Note
\end{itemize}
\end{proof}

















\begin{problem}{2.19}
If $f\left(x,y\right) = x^{x^{x^y}} + \left(\log x\right) \left(\arctan \left(\arctan\left(\arctan \left(\sin \left(\cos xy\right)-\log \left(x+y\right)\right)\right)\right) \right)$ find $D_2f\left(1,y\right)$. \textit{Hint}: There is an easy way to do this.
\end{problem}
\begin{proof}
At $x=1$, the function to differentiate is $f\left(1,y\right) = 0$, and so the derivative is $0$, since the function is constant.
\end{proof}







\begin{problem}{2.20}
Find the partial derivatives of $f$ in terms of the derivatives of $g$ and $h$  if
\begin{itemize}
	\item $f\left(x,y\right) = f\left(x\right)h\left(y\right) $
	\item $f\left(x,y\right)=g\left(x\right)^{h\left(y\right)}$
	\item $f\left(x,y\right)=g\left(x\right)$
	\item $f\left(x,y\right)=g\left(y\right)$
	\item $f\left(x,y\right)=g\left(x+y\right)$
\end{itemize}
\end{problem}
\begin{proof}
\begin{itemize}
	\item $D_1 f\left(x,y\right) = h\left(y\right)f'\left(x\right)$\\ $D_2f\left(x,y\right) = f\left(x\right)h'\left(y\right)$
	\item $D_1 f\left(x,y\right) = g'\left(x\right) h\left(y\right) g\left(x\right)^{h\left(y\right)-1}$\\ $D_2 f\left(x,y\right) = \log\left(g\left(x\right)\right) h'\left(y\right) g\left(x\right)^{h\left(y\right)}$
	\item $D_1 f\left(x,y\right) = g'\left(x\right)$\\ $D_2 f\left(x,y\right) = 0$
	\item $D_1 f\left(x,y\right) = 0$\\ $D_2 f\left(x,y\right) = g'\left(y\right)$
	\item $D_1 f\left(x,y\right) = g'\left(x+y\right)$\\ $D_2 f\left(x,y\right) = g'\left(x+y\right)$
\end{itemize}
\end{proof}



\begin{problem}{2-21}
Let $g_1,g_2:\mathbb{R}^2\rightarrow \mathbb{R}$ be continuous. Define $f:\mathbb{R}^2\rightarrow \mathbb{R}$ by $$ f\left(x,y\right) = \int_0^x g_1\left(t,0\right)\, dt + \int_0^y g_2\left(x,y\right)\, dt $$
\begin{itemize}
	\item Show that $D_2 f\left(x,y\right) = g_2 \left(x,y\right)$.
	\item How should $f$ be defined so that $D_1 f\left(x,y\right) = g_1\left(x,y\right)$?
	\item Find a function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ such that $D_1 f\left(x,y\right)=x$ and $D_2f\left(x,y\right)=y$. Find one such that $D_1 f\left(x,y\right)=y$ and $D_2 f\left(x,y\right)=x$.
\end{itemize}
\end{problem}

\begin{proof}
\begin{itemize}
	\item Since the first integral in the sum is a function only of $x$, it vanishes under the derivative $D_2$. The second integral is differentiated via the second fundamental theorem of calculus: $$ D_2 f\left(x,y\right) = 0 + g_2 \left(x,y\right) = g_2\left(x,y\right).$$
	\item By the symmetry of this problem, it is easy to see that $f$ should be defined in this case as $$ f\left(x,y\right) = \int_0^x g_1\left(t,y\right)\, dt + \int_0^y g_2\left(x,0\right)\, dt $$ and a similar calculation as in the first part of this proof confirms this to work.
	\item Firstly, the function $f\left(x,y\right) = \frac{1}{2}\left(x^2 + y^2\right)$ has partial derivatives $D_1 f\left(x,y\right) = x$ and $D_2 f\left(x,y\right) = y$. Next, the function $f\left(x,y\right) = xy$ has partial derivatives $D_1 f\left(x,y\right) = y$ and $D_2f\left(x,y\right) = x$.
\end{itemize}
\end{proof}





\begin{problem}{2-22}
If $f:\mathbb{R}^2\rightarrow \mathbb{R}$ and $D_2 f = 0$, show that $f$ is independent of the second variable. If $D_1 f= D_2 f=0$, show that $f$ is constant.
\end{problem}
\begin{proof}
Assume $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ and $D_2 f=0$. The usual mean value theorem of univariate calculus applies to the function $g\left(y\right)=f\left(x,y\right)$, which has derivative $g'\left(y\right) = D_2 f\left(x,y\right) = 0$; hence $g\left(y\right) = f\left(x,y\right)$ is constant (with respect to $y$) on its domain, and so it is independent of the second variable.
\end{proof}






\begin{problem}{2-23}
Let $A=\left\{\left(x,y\right)\in \mathbb{R}^2 \, | \, x<0, \; \text{or} \; x\geq 0 \; \text{and} \; y\neq 0\right\}$.
\begin{itemize}
	\item If $f:A\rightarrow \mathbb{R}$ and $D_1 f = D_2 f = 0$, show that $f$ is constant. \textit{Hint}: Note that any two points in $A$ can be connected by a sequence of lines each parallel to one of the axes.
	\item Find a function $f:A\rightarrow \mathbb{R}$ such that $D_2 f=0$ but $f$ is not independent of the second variable.
\end{itemize}
\end{problem}





\begin{problem}{2-24}
Define $f:\mathbb{R}^2\rightarrow \mathbb{R}$ by $$f\left(x,y\right) = \left\{ \begin{array}{lr} xy\frac{x^2-y^2}{x^2+y^2} & \left(x,y\right)\neq 0, \\ 0 & \left(x,y\right)=0. \end{array}\right. $$
\end{problem}




% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 
\end{document}